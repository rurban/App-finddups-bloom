#!/usr/bin/perl -s
# Reini Urban rurban@cpanel.net 2014
# This does not qualify for the regular dfw contest entry, but is more practical,
# more in the spirit of a coreutil and faster.
package App::finddups::bloom;

use strict;
use vars qw($d $h $0);
our $VERSION = '0.01';

# local::lib hack, dfw-specific only
# push @INC, "perl5/lib/perl5:perl5/lib/perl5/x86_64-linux-gnu-thread-multi";

=head1 NAME

App::finddups::bloom

=head1 SYNOPSIS

    finddups-bloom      # acting on /dedup

    finddups-bloom /dir
    finddups-bloom /dir | xargs rm        # printable filenames without spaces only

    finddups-bloom -0 /dir | xargs -0 rm  # quote \0 and filenames containing space

=head1 DESCRIPTION

Finds and prints most file duplicates under the given directory.
Ignores symlinks and hardlinks.

=cut

use File::Find;
use Bloom::Faster;
use Digest::CRC;

if ($h) {
  require Pod::Usage;
  Pod::Usage->import('pod2usage');
  pod2usage( -verbose => 0 );
}

my $root = shift || "/dedup";
my $size = new Bloom::Faster({n => 100_000, e => 0.01}); # expected elements + error rate (not using options yet)
my $hash = new Bloom::Faster({n => 2000, e => 0.01});    # less expected same-size entries
my $crc = Digest::CRC->new(type=>"crc64");
my $s; # global crc buffer

print "# finding most likely duplicates in $root\n" if $d;

sub wanted {
  return if !-f $_ or -l $_ ;  # skip symlinks and non-files
  return if (stat($_))[3] > 1; # also skip hardlinks not using a inode hash, the fs already stores nlinks
  if ($size->add(-s _)) {      # only compare same filesizes
    my $c;
    open my $f,'<',$_;
    if (-s _ < 1_000_000) {
      $c = $crc->addfile($f);
    } else {
      sysread $f, $s, 1_000_000; # only check the first 1 million bytes
      $c = $crc->add($s);
    }
    if ($hash->add($c)) {
      print $File::Find::name."\n";
    }
    close $f;
  }
}

find(\&wanted, $root) unless caller;

END {
  if ($d) {
    print "\n#bloom filter stats: ", $size->key_count, " size keys, ", $size->capacity, " size capacity\n";
    print   "#                    ", $hash->key_count, " hash keys, ", $hash->capacity, " hash capacity\n";
  }
}

=head1 INTERNALS

This code doesn't follow the contest rules, as it doesn't store the source links. 
It only checks and prints duplicates, and therefore can use fast and memory efficient
Bloom filters.

The accuracy is 99.99% which should satisfy all practical purposes,
and if you care for more run it a second time, combined with rm.
It is fast because it uses bloom filters not hashes, hence the 99.99%
and not 100% and it doesn't print the source for the duplicates, only the duplicates.

To remove the duplicates apply C<rm> to the list of entries, such as

    finddups-bloom | xargs rm

If you care about filenames which might contain \0 characters use the option -0
similar to find and xargs from findutils

=head1 LICENSE

Written and copyright 2014 by Reini Urban rurban@cpanel.net
for the Dallas/Fort Worth Perl Mongers dfw dedup contest 2014
http://dfw.pm.org/

This program is free software; you can redistribute it and/or modify
it under the terms of Perl5, which is either:

a) the GNU General Public License as published by the Free
   Software Foundation; either version 1, or (at your option) any
   later version, or

b) the "Artistic License" which comes with this kit.

